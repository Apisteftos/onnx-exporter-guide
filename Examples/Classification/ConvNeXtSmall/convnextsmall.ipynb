{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting ConvNeXt Small from Pytorch to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision.models import convnext_small, ConvNeXt_Small_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ConvNeXt_Small_Weights.IMAGENET1K_V1\n",
    "convnext_small = convnext_small(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a random input tensor\n",
    "<br>\n",
    "ConvNeXt base mode is batched (B, C, H, W) and according to Pytocrh docs, the images are resized to 230 and croped into 224. So it is important to initialize the model with 224x224 dims, but resizing with 230x230 and croping it in 224x224 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model to onnx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(convnext_small,\n",
    "                  input, \n",
    "                  \"convnextsmall.onnx\",\n",
    "                  input_names=[\"input\"],\n",
    "                  output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input names: input\n",
      "Output names: output\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "model = ort.InferenceSession(\"convnextsmall.onnx\")\n",
    "\n",
    "print(\"Input names:\", model.get_inputs()[0].name)\n",
    "print(\"Output names:\", model.get_outputs()[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Using Netron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"input_convnextsmall.png\"  width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"output_convnextbase.png\"  width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ConvNeXt base on the CPU involves severar implementation steps\n",
    "\n",
    "- Preparation of the input tensor (preproceccing)\n",
    "- Performing an inference in the CPU accelerator\n",
    "- Converting class probabilities to class labels (postprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(image): \n",
    "    \n",
    "    img = cv2.resize(image, (230, 230))\n",
    "    height, width, _ = img.shape\n",
    "    x = (width - 224)//2\n",
    "    y = (height - 224)//2\n",
    "    img = img[y:y+224, x:x+224]\n",
    "    img = np.asarray(img, np.float32)\n",
    "    img = np.transpose(img, [2, 0, 1])\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    for i in range(3):\n",
    "        img[i,:,:] = (img[i,:,:]/255 - mean[i] / std[i])\n",
    "    \n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "img = \"peacock.jpg\"\n",
    "img = cv2.imread(img)\n",
    "img = preprocessing(img)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session = ort.InferenceSession(\"convnextsmall.onnx\", session_options, [\"CPUExecutionProvider\"])\n",
    "\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "ortvalue = ort.OrtValue.ortvalue_from_numpy(img)\n",
    "session = session.run([output_name], {input_name:ortvalue})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: peacock\n",
      "Predicted Index: 84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_classes(filepath):\n",
    "    with open(filepath, 'r') as file: \n",
    "        classes = [line.strip() for line in file]\n",
    "    return classes\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def postprocessing(data, classes):\n",
    "    \n",
    "    data = np.exp(data - np.max(data))\n",
    "    prob = data/data.sum()\n",
    "    pred_index = np.argmax(prob)\n",
    "    predicted_class = classes[pred_index]\n",
    "    \n",
    "    return predicted_class, pred_index\n",
    "    \n",
    "\n",
    "classes = read_classes(\"classes.txt\")\n",
    "\n",
    "predicted_class, predicted_index = postprocessing(session, classes)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)\n",
    "print(\"Predicted Index:\", predicted_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of ResNet152 on the GPU. In case you installed `onnxruntime`, you need uninstall onnxruntime and install `pip install onnxruntime-gpu`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: peacock\n",
      "Predicted Index: 84\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import torch\n",
    "\n",
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_gpu = ort.InferenceSession(\"convnextsmall.onnx\", session_options, providers)\n",
    "input_name = session_gpu.get_inputs()[0].name\n",
    "output_name = session_gpu.get_outputs()[0].name\n",
    "io_binding = session_gpu.io_binding()\n",
    "ortvalue = ort.OrtValue.ortvalue_from_numpy(img, 'cuda', 0)\n",
    "io_binding.bind_input(name=input_name,\n",
    "                        device_type='cuda',\n",
    "                        device_id=0,\n",
    "                        element_type=np.float32,\n",
    "                        shape = ortvalue.shape(),\n",
    "                        buffer_ptr=ortvalue.data_ptr()\n",
    "                        )\n",
    "\n",
    "io_binding.bind_output(name= output_name,\n",
    "                             device_type='cuda',\n",
    "                             device_id=0)\n",
    "\n",
    "session_gpu.run_with_iobinding(io_binding)\n",
    "\n",
    "output = io_binding.copy_outputs_to_cpu()\n",
    "predicted_class, predicted_index = postprocessing(output, classes)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)\n",
    "print(\"Predicted Index:\", predicted_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
