{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting ConvNeXt Large from Pytorch to ONNX format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchvision.models import convnext_large, ConvNeXt_Large_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-traned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ConvNeXt_Large_Weights.IMAGENET1K_V1\n",
    "convnext_large = convnext_large(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a random input tensor\n",
    "<br>\n",
    "ConvNeXt base mode is batched (B, C, H, W) and according to Pytocrh docs, the images are resized to 232 and croped into 224. So it is important to initialize the model with 224x224 dims, but resizing with 232x232 and croping it in 224x224 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export model to onnx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(convnext_large,\n",
    "                  input,\n",
    "                  \"convnextlarge.onnx\",\n",
    "                  input_names=[\"input\"],\n",
    "                  output_names=[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input names: input\n",
      "Output names: output\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "model = ort.InferenceSession(\"convnextlarge.onnx\")\n",
    "\n",
    "print(\"Input names:\", model.get_inputs()[0].name)\n",
    "print(\"Output names:\", model.get_outputs()[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification Using Netron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"input_convnextlarge.png\"  width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"output_convnextlarge.png\"  width=\"1000\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ConvNeXt large on the CPU involves severar implementation steps\n",
    "\n",
    "- Preparation of the input tensor (preproceccing)\n",
    "- Performing an inference in the CPU accelerator\n",
    "- Converting class probabilities to class labels (postprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(image): \n",
    "    \n",
    "    img = cv2.resize(image, (232, 232))\n",
    "    height, width, _ = img.shape\n",
    "    x = (width - 224)//2\n",
    "    y = (height - 224)//2\n",
    "    img = img[y:y+224, x:x+224]\n",
    "    img = np.asarray(img, np.float32)\n",
    "    img = np.transpose(img, [2, 0, 1])\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    for i in range(3):\n",
    "        img[i,:,:] = (img[i,:,:]/255 - mean[i] / std[i])\n",
    "    \n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "img = \"snake.jpg\"\n",
    "img = cv2.imread(img)\n",
    "img = preprocessing(img)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session = ort.InferenceSession(\"convnextlarge.onnx\", session_options, [\"CPUExecutionProvider\"])\n",
    "\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "ortvalue = ort.OrtValue.ortvalue_from_numpy(img)\n",
    "session = session.run([output_name], {input_name:ortvalue})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: boa constrictor\n",
      "Predicted Index: 61\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_classes(filepath):\n",
    "    with open(filepath, 'r') as file: \n",
    "        classes = [line.strip() for line in file]\n",
    "    return classes\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def postprocessing(data, classes):\n",
    "    \n",
    "    data = np.exp(data - np.max(data))\n",
    "    prob = data/data.sum()\n",
    "    pred_index = np.argmax(prob)\n",
    "    predicted_class = classes[pred_index]\n",
    "    \n",
    "    return predicted_class, pred_index\n",
    "    \n",
    "\n",
    "classes = read_classes(\"classes.txt\")\n",
    "\n",
    "predicted_class, predicted_index = postprocessing(session, classes)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)\n",
    "print(\"Predicted Index:\", predicted_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of ResNet152 on the GPU. In case you installed `onnxruntime`, you need uninstall onnxruntime and install `pip install onnxruntime-gpu`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: boa constrictor\n",
      "Predicted Index: 61\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import torch\n",
    "\n",
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "\n",
    "\n",
    "\n",
    "session_options = ort.SessionOptions()\n",
    "session_gpu = ort.InferenceSession(\"convnextlarge.onnx\", session_options, providers)\n",
    "input_name = session_gpu.get_inputs()[0].name\n",
    "output_name = session_gpu.get_outputs()[0].name\n",
    "io_binding = session_gpu.io_binding()\n",
    "ortvalue = ort.OrtValue.ortvalue_from_numpy(img, 'cuda', 0)\n",
    "io_binding.bind_input(name=input_name,\n",
    "                        device_type='cuda',\n",
    "                        device_id=0,\n",
    "                        element_type=np.float32,\n",
    "                        shape = ortvalue.shape(),\n",
    "                        buffer_ptr=ortvalue.data_ptr()\n",
    "                        )\n",
    "\n",
    "io_binding.bind_output(name= output_name,\n",
    "                             device_type='cuda',\n",
    "                             device_id=0)\n",
    "\n",
    "session_gpu.run_with_iobinding(io_binding)\n",
    "\n",
    "output = io_binding.copy_outputs_to_cpu()\n",
    "predicted_class, predicted_index = postprocessing(output, classes)\n",
    "\n",
    "print(\"Predicted Class:\", predicted_class)\n",
    "print(\"Predicted Index:\", predicted_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
